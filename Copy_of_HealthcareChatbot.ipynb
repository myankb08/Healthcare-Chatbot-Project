{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myankb08/Healthcare-Chatbot-Project/blob/main/Copy_of_HealthcareChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqIzgXkw4raA"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langgraph sentence-transformers transformers accelerate bitsandbytes gliner medspacy spacy fastapi uvicorn faiss-cpu --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em_zFckwmNuE"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# Install Dependencies\n",
        "# ======================\n",
        "!pip install langchain langchain-community langchain-core langgraph sentence-transformers transformers accelerate bitsandbytes gliner annoy --quiet\n",
        "\n",
        "# ======================\n",
        "# Imports\n",
        "# ======================\n",
        "import pickle\n",
        "import torch\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Annoy\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langgraph.graph import StateGraph, END\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationEntityMemory, InMemoryEntityStore, VectorStoreRetrieverMemory\n",
        "from gliner import GLiNER\n",
        "\n",
        "# ======================\n",
        "# ---- Pickle Setup ----\n",
        "# ======================\n",
        "MEMORY_FILE = \"conversation_memory.pkl\"\n",
        "ENTITY_FILE = \"entity_memory.pkl\"\n",
        "\n",
        "def load_pickle(path, default):\n",
        "    try:\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        return default\n",
        "\n",
        "def save_pickle(path, obj):\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def load_memory():\n",
        "    return load_pickle(MEMORY_FILE, [])\n",
        "\n",
        "def save_memory(memory):\n",
        "    save_pickle(MEMORY_FILE, memory)\n",
        "\n",
        "def load_entities():\n",
        "    return load_pickle(ENTITY_FILE, {})\n",
        "\n",
        "def save_entities(entities):\n",
        "    save_pickle(ENTITY_FILE, entities)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- State ----\n",
        "# ======================\n",
        "class State(dict):\n",
        "    messages: list\n",
        "    entities: dict\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- HuggingFace LLM ----\n",
        "# ======================\n",
        "model_name = \"unsloth/phi-3-mini-4k-instruct-bnb-4bit\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- Structured Entity Memory ----\n",
        "# ======================\n",
        "entity_store = InMemoryEntityStore()\n",
        "entity_memory = ConversationEntityMemory(llm=llm, entity_store=entity_store)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- GLiNER Entity Extractor ----\n",
        "# ======================\n",
        "gliner_model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
        "\n",
        "GLINER_LABELS = [\n",
        "    \"PERSON\", \"GPE\", \"ORG\", \"DATE\", \"MEDICAL\",\n",
        "    \"CONDITION\", \"SYMPTOM\", \"MEDICATION\", \"PROCEDURE\", \"ANATOMY\", \"INJURY\"\n",
        "]\n",
        "\n",
        "def extract_gliner_entities(text: str):\n",
        "    preds = gliner_model.predict_entities(text, GLINER_LABELS, threshold=0.5)\n",
        "    entities = {}\n",
        "    for ent in preds:\n",
        "        label = ent[\"label\"]\n",
        "        entities.setdefault(label, set()).add(ent[\"text\"])\n",
        "    return {k: sorted(list(v)) for k, v in entities.items()}\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- Vector Memory (FAISS in-memory only) ----\n",
        "# ======================\n",
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = None\n",
        "retriever = None\n",
        "vector_memory = None\n",
        "\n",
        "# ======================\n",
        "# ---- Knowledge Base ----\n",
        "# ======================\n",
        "KNOWLEDGE_BASE = {\n",
        "    # General Health FAQs\n",
        "    \"symptoms of flu\": \"Common flu symptoms include fever, cough, sore throat, body aches, headaches, and fatigue.\",\n",
        "    \"symptoms of a common cold\": \"Common cold symptoms include a runny or stuffy nose, sore throat, sneezing, and a mild cough.\",\n",
        "    \"what is dehydration\": \"Dehydration occurs when your body loses more fluid than you take in. Symptoms include thirst, dark yellow urine, dizziness, and fatigue.\",\n",
        "    \"how to treat a minor burn\": \"For a minor burn, run cool water over the area for several minutes. Do not use ice. After cooling, you can apply lotion or an antibiotic ointment and cover it with a clean bandage.\",\n",
        "\n",
        "    # Healthy Lifestyle Tips\n",
        "    \"tips for better sleep\": \"To improve sleep, stick to a regular sleep schedule, create a restful environment, limit caffeine and large meals before bed, and get some exercise during the day.\",\n",
        "    \"benefits of a balanced diet\": \"A balanced diet provides essential nutrients, helps maintain a healthy weight, supports your immune system, and reduces the risk of chronic diseases.\",\n",
        "    \"how much water to drink\": \"Most adults should aim to drink about 8 glasses (around 2 liters) of water per day, but this can vary based on activity level and climate.\",\n",
        "\n",
        "    # Hospital Information\n",
        "    \"hospital working hours\": \"Our hospital is open 24/7 for emergency services. The outpatient department (OPD) is open from 9 AM to 5 PM, Monday to Saturday.\",\n",
        "    \"how to book an appointment\": \"You can book an appointment by using the online portal on IITGn Medical Center's website (https://hcrs.iitgn.ac.in/slotbooking/).\",\n",
        "    \"what is the hospital contact number\": \"For emergencies, please call +91 - 70 69 79 5000. For general inquiries, our landline number is 079-2395-1116.\",\n",
        "    \"where is the hospital located\": \"We are located at Central Arcade First Floor.\"\n",
        "}\n",
        "\n",
        "# ======================\n",
        "# ---- Searchable Knowledge Index ----\n",
        "# ======================\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Check if the knowledge base is not empty\n",
        "if KNOWLEDGE_BASE:\n",
        "    # Get the questions (keys) and answers (values) from the knowledge base\n",
        "    knowledge_base_texts = list(KNOWLEDGE_BASE.keys())\n",
        "    knowledge_base_metadatas = [{\"answer\": answer} for answer in KNOWLEDGE_BASE.values()]\n",
        "\n",
        "    # Create a FAISS index from the knowledge base questions\n",
        "    knowledge_base_index = FAISS.from_texts(\n",
        "        texts=knowledge_base_texts,\n",
        "        embedding=embedder,  # Using the same embedder as the conversation memory\n",
        "        metadatas=knowledge_base_metadatas\n",
        "    )\n",
        "    print(\"✅ Knowledge Base Index created successfully.\")\n",
        "else:\n",
        "    knowledge_base_index = None\n",
        "    print(\"⚠️ Knowledge Base is empty. Skipping index creation.\")\n",
        "\n",
        "\n",
        "def find_relevant_knowledge(user_input: str) -> str:\n",
        "    \"\"\"Performs a semantic search on the knowledge base index.\"\"\"\n",
        "    if knowledge_base_index:\n",
        "        # Search for the most similar question in the index\n",
        "        # It returns the document and its similarity score (0 to 1, lower is better)\n",
        "        results = knowledge_base_index.similarity_search_with_score(user_input, k=1)\n",
        "        if results and results[0][1] < 0.5:  # Using a threshold of 0.5\n",
        "            # If a relevant document is found with a good score, return its answer\n",
        "            return results[0][0].metadata[\"answer\"]\n",
        "    return \"\"\n",
        "\n",
        "# ======================\n",
        "# ---- Nodes ----\n",
        "# ======================\n",
        "def add_message(state: State, config: RunnableConfig):\n",
        "    \"\"\"Persist conversation + entities + vector memory (in RAM only).\"\"\"\n",
        "    global vectordb, retriever, vector_memory\n",
        "\n",
        "    memory = load_memory()\n",
        "    memory.extend(state[\"messages\"])\n",
        "    save_memory(memory)\n",
        "    save_entities(state.get(\"entities\", {}))\n",
        "\n",
        "    # Get the text from the latest human message to add to the vector store\n",
        "    new_text = state[\"messages\"][-1].content if isinstance(state[\"messages\"][-1], HumanMessage) else None\n",
        "\n",
        "    if new_text:\n",
        "        if vectordb is None:\n",
        "            # Create the vector store for the first time\n",
        "            vectordb = FAISS.from_texts([new_text], embedding=embedder)\n",
        "            retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "            vector_memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
        "        else:\n",
        "            # Add the new text to the existing FAISS store\n",
        "            vectordb.add_texts([new_text])\n",
        "\n",
        "    return {\"messages\": memory, \"entities\": state.get(\"entities\", {})}\n",
        "\n",
        "def extract_entities(state: State, config: RunnableConfig):\n",
        "    \"\"\"Hybrid entity extraction with GLiNER + LLM memory.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    if not messages:\n",
        "        return state\n",
        "    last_input = messages[-1].content\n",
        "    last_output = \"\"\n",
        "    if len(messages) >= 2 and isinstance(messages[-1], AIMessage):\n",
        "        last_output = messages[-1].content\n",
        "    entity_memory.save_context({\"input\": last_input}, {\"output\": last_output})\n",
        "    llm_entities = entity_store.store\n",
        "    gliner_entities = extract_gliner_entities(last_input)\n",
        "    merged = load_entities()\n",
        "    for k, v in llm_entities.items():\n",
        "        merged.setdefault(k, [])\n",
        "        merged[k] = list(set(merged.get(k, [])) | set(v))\n",
        "    for k, v in gliner_entities.items():\n",
        "        merged.setdefault(k, [])\n",
        "        merged[k] = list(set(merged.get(k, [])) | set(v))\n",
        "    return {\"messages\": messages, \"entities\": merged}\n",
        "\n",
        "\n",
        "def call_llm(state: State, config: RunnableConfig):\n",
        "    \"\"\"Call HuggingFace LLM with a properly formatted chat prompt.\"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    entities = state.get(\"entities\", {})\n",
        "    user_input = state.get(\"last_input\", messages[-1].content if messages else \"\")\n",
        "\n",
        "    # 1. Search the Knowledge Base\n",
        "    knowledge = find_relevant_knowledge(user_input)\n",
        "\n",
        "    # 2. Retrieve vector memory\n",
        "    retrieved_context = \"\"\n",
        "    if vector_memory:\n",
        "        retrieved_context = vector_memory.load_memory_variables(\n",
        "            {\"prompt\": user_input}\n",
        "        ).get(\"history\", \"\")\n",
        "\n",
        "    # --- NEW: Create a structured chat prompt ---\n",
        "    # System message sets the persona and instructions\n",
        "    system_message = (\n",
        "        f\"You are a helpful assistant for the IIT Gandhinagar Health Centre.\\n\"\n",
        "        f\"Answer the user's question based on the following context. If the answer is not in the context, use your general knowledge but maintain the persona of a health center assistant.\\n\"\n",
        "        f\"When a user provides personal details like age, use them to confirm or refine the general information if possible. For example, if general advice applies to their demographic, confirm that for them. Maintain a helpful tone.\\n\"\n",
        "        f\"Do not give direct medical advice. Avoid simply repeating 'consult a doctor' unless the user is asking for a diagnosis or treatment for a serious issue.\\n\\n\"\n",
        "        f\"--- CONTEXT ---\\n\"\n",
        "        f\"Knowledge Base Info: {knowledge}\\n\"\n",
        "        f\"Previous Conversation Snippets: {retrieved_context}\\n\"\n",
        "        f\"Known Entities: {entities}\\n\"\n",
        "        f\"--- END CONTEXT ---\"\n",
        "    )\n",
        "\n",
        "    # Create a message list for the template\n",
        "    prompt_messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "    ]\n",
        "\n",
        "    # Use the tokenizer to apply the model's specific chat template\n",
        "    formatted_prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    # Invoke the LLM with the correctly formatted prompt\n",
        "    response = llm.invoke(formatted_prompt)\n",
        "    assistant_reply = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"messages\": messages + [AIMessage(content=assistant_reply)],\n",
        "        \"entities\": entities\n",
        "    }\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- Graph ----\n",
        "# ======================\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"chat\", call_llm)\n",
        "workflow.add_node(\"extract\", extract_entities)\n",
        "workflow.add_node(\"save_message\", add_message)\n",
        "\n",
        "workflow.set_entry_point(\"chat\")\n",
        "workflow.add_edge(\"chat\", \"extract\")\n",
        "workflow.add_edge(\"extract\", \"save_message\")\n",
        "workflow.add_edge(\"save_message\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# ---- Usage ----\n",
        "# ======================\n",
        "# user_input = \"\"\"chiefComplaint Follow-up case of pain and swelling of the left wrist\n",
        "# after he allegedly slipped and fell while washing for prayer on February 4, 2025,\n",
        "# with a fracture of the distal radius.\n",
        "# Other complaint: Pain in the left wrist after lifting a heavy weight at work on April 6, 2025.\n",
        "# Follow-up case of pain and swelling of the left wrist after he allegedly slipped and fell.\"\"\"\n",
        "# state = {\"messages\": [HumanMessage(content=user_input)], \"entities\": load_entities()}\n",
        "# output = app.invoke(state)\n",
        "\n",
        "# print(\"AI:\", output[\"messages\"][-1].content)\n",
        "# print(\"Entities Extracted:\", output[\"entities\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbokHpN0sAtz"
      },
      "outputs": [],
      "source": [
        "# # ======================\n",
        "# # ---- Interactive Conversation Loop ----\n",
        "# # ======================\n",
        "# state = {\"messages\": load_memory(), \"entities\": load_entities()}\n",
        "\n",
        "# print(\"💬 Starting conversation with the agent (type 'exit' to quit)\")\n",
        "\n",
        "# while True:\n",
        "#     user_input = input(\"You: \")\n",
        "#     if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "#         break\n",
        "\n",
        "#     # Append user message\n",
        "#     state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "#     # Run through graph\n",
        "#     # ✅ Always pass the latest input explicitly for vector memory\n",
        "#     state = app.invoke(\n",
        "#         {**state, \"last_input\": user_input}  # added key for retrieval\n",
        "#     )\n",
        "\n",
        "#     # Get AI response\n",
        "#     ai_response = state[\"messages\"][-1].content\n",
        "#     print(\"AI:\", ai_response)\n",
        "#     print(\"Entities Extracted:\", state[\"entities\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7G17fFv1Ilc"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# ---- Gradio UI ----\n",
        "# ======================\n",
        "import gradio as gr\n",
        "\n",
        "# Load initial state once\n",
        "initial_state = {\"messages\": load_memory(), \"entities\": load_entities()}\n",
        "\n",
        "def chat_interface(user_input, history):\n",
        "    # Append the new user message to the history\n",
        "    initial_state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "\n",
        "    # Run the graph\n",
        "    final_state = app.invoke({\n",
        "        \"messages\": initial_state[\"messages\"],\n",
        "        \"entities\": initial_state[\"entities\"],\n",
        "        \"last_input\": user_input\n",
        "    })\n",
        "\n",
        "    # Update the persistent state\n",
        "    initial_state[\"messages\"] = final_state[\"messages\"]\n",
        "    initial_state[\"entities\"] = final_state[\"entities\"]\n",
        "\n",
        "    # Get the latest AI response\n",
        "    ai_response = final_state[\"messages\"][-1].content\n",
        "\n",
        "    # Gradio expects a list of tuples for history: (user_msg, ai_msg)\n",
        "    history.append((user_input, ai_response))\n",
        "    return \"\", history\n",
        "\n",
        "\n",
        "# Build the Gradio app\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🏥 IITGn Healthcare Information Assistant\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Ask a question about health or hospital services\")\n",
        "    clear = gr.Button(\"Clear Conversation\")\n",
        "\n",
        "    # --- NEW: Added a static disclaimer to the UI ---\n",
        "    gr.Markdown(\n",
        "        \"Disclaimer: This is for educational purposes only and not a substitute for professional medical advice.\"\n",
        "    )\n",
        "\n",
        "    msg.submit(chat_interface, [msg, chatbot], [msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}